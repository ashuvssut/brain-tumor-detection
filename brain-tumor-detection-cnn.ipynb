{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mode is set to false to avoid unwated testing outputs from the cells\n",
    "test = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brain tumor dataset is taken from `./brain_tumor_dataset/yes` and `brain_tumor_dataset/no` directories. \n",
    "> **Dataset Source:** https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection \n",
    "> \n",
    "> **Dataset Description**: The dataset contains 2 folders (yes/no) and each folder contains subfolders with several images. There are 253 images with brain tumor and 98 images without brain tumor. The images are in .jpg format. The image sizes are not consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YES_DIR_PATH = 'brain_tumor_dataset/yes/'\n",
    "NO_DIR_PATH = 'brain_tumor_dataset/no/'\n",
    "yes_imgs_name = os.listdir(YES_DIR_PATH)\n",
    "no_imgs_name = os.listdir(NO_DIR_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing the images\n",
    "\n",
    "  1. Crop the brain out of the images.\n",
    "  2. Resize the images to a standard size of 32x32 for easier processing.\n",
    "  3. Normalize pixels of the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cropping the brain out of the images\n",
    "Create a function to crop the brain out an given image\n",
    "\n",
    "![MRI Cropping Procedure](./MRIcropping.png)\n",
    "<!-- For kaggle drag and drop MRIcropping.png into the notebook markdown -->\n",
    "\n",
    "  0. Resize the image to 256x256 pixels\n",
    "  1. Convert the image to grayscale\n",
    "  2. Apply a Gaussian blur to the image\n",
    "  3. Threshold the image and perform a series of erosions and dilations to remove any small regions of noise\n",
    "  4. Find the contours of the brain\n",
    "  5. Find the largest contour\n",
    "  6. Crop the image to the bounding box of the largest contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    # Resize the image to 256x256 pixels\n",
    "    resized_img = cv2.resize(\n",
    "        img,\n",
    "        dsize=(256, 256),\n",
    "        interpolation=cv2.INTER_CUBIC\n",
    "    )\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(resized_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Apply a Gaussian blur to the image\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Threshold the image by Binary Thresholding\n",
    "    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # perform a series of erosions & dilations to remove any small regions of noise\n",
    "    thresh = cv2.erode(thresh, None, iterations=2)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    # find contours in thresholded image, then grab the largest one\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "                            cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "    # find the extreme points\n",
    "    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "    extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "    # crop\n",
    "    ADD_PIXELS = 0\n",
    "    cropped_img = resized_img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS,\n",
    "                              extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
    "    return cropped_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use crop_image function to crop the images\n",
    "yes_imgs_cropped = [crop_image(cv2.imread(YES_DIR_PATH + img_file)) for img_file in yes_imgs_name]\n",
    "no_imgs_cropped = [crop_image(cv2.imread(NO_DIR_PATH + img_file)) for img_file in no_imgs_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Resize the cropped images\n",
    "\n",
    "The cropped images are resized to 32x32 for easier processing.\n",
    "\n",
    "The `resized_imgs` list contains the resized images of the `yes_imgs_cropped` and `no_imgs_cropped` lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_imgs = yes_imgs_cropped + no_imgs_cropped\n",
    "resized_imgs = [cv2.resize(img, dsize=(32, 32), interpolation=cv2.INTER_CUBIC) for img in orig_imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.squeeze(resized_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (test):\n",
    "    print(type(X))\n",
    "    print(X.shape)\n",
    "    print(X)\n",
    "    print(resized_imgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Normalize pixels of the images\n",
    "Normalize each pixel of the images present in the numpy array `X` and save it as `X`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "X = X.astype('float32')\n",
    "X /= 255\n",
    "\n",
    "if (test):\n",
    "    print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating and Training the CNN model\n",
    "  1. Create the labels for the images\n",
    "  2. Prepare the training and validation/testing data\n",
    "  3. Create the CNN model and visualize the model summary\n",
    "  4. Compile the model\n",
    "  5. Train the model on the training data\n",
    "  6. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Create the labels for the images\n",
    "The `img_labels` list stores the labels for each image. The labels are 0 for \"no\" and 1 for \"yes\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_yes = np.full(len(yes_imgs_name), 1)\n",
    "labels_no = np.full(len(no_imgs_name), 0)\n",
    "\n",
    "img_labels = np.concatenate([labels_yes, labels_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (test):\n",
    "    print(img_labels.size, img_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare the training and validation/testing data\n",
    "\n",
    "Prepare `x_train` and `x_valid` from the `X` image data list.\n",
    "Prepare `y_train` and `y_valid` from the `img_labels` list.\n",
    "\n",
    "As stated above, the dataset is divided into 2 directories: \"yes\" & \"no\". There are 253 images (155 images for \"yes\" and 98 images for \"no\"). We were storing the labels for each image in the `img_labels` list. The labels are 0 for \"no\" and 1 for \"yes\".\n",
    "\n",
    "So we are separately storing the images of \"yes\" and \"no\" in the `yes_imgs` and `no_imgs` lists respectively and thier labels in the `yes_labels` and `no_labels` lists respectively.\n",
    "\n",
    "We will divide those images into training and validation sets. We will use 80% of the images for training and 20% of the images for validation. So we will use 202 images for training and 51 images for validation. Out of 202 images, 124 images are of \"yes\" and 78 images are of \"no\". Similarly, out of 51 images, 31 images are of \"yes\" and 20 images are of \"no\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set (i.e `x_train`) and Testing set/Validation set (i.e `x_valid`)\n",
    "# We will also keep the original images of the validation set in `x_orig_valid` for visualization purposes\n",
    "yes_imgs = X[:155]\n",
    "no_imgs = X[155:]\n",
    "yes_orig_imgs = orig_imgs[:155]\n",
    "no_orig_imgs = orig_imgs[155:]\n",
    "\n",
    "x_yes_train = yes_imgs[:124]\n",
    "x_yes_valid = yes_imgs[124:]\n",
    "x_yes_orig_valid = yes_orig_imgs[124:]\n",
    "\n",
    "x_no_train = no_imgs[:78]\n",
    "x_no_valid = no_imgs[78:]\n",
    "x_no_orig_valid = no_orig_imgs[78:]\n",
    "\n",
    "x_train = np.concatenate([x_yes_train, x_no_train]) # 80% of the images\n",
    "x_valid = np.concatenate([x_yes_valid, x_no_valid]) # 20% of the images\n",
    "x_orig_valid = np.concatenate([x_yes_orig_valid, x_no_orig_valid])\n",
    "\n",
    "# Splitting the dataset labels for the Training set (i.e `y_train`) and Testing set/Validation set (i.e `y_valid`)\n",
    "yes_labels = img_labels[:155]\n",
    "no_labels = img_labels[155:]\n",
    "\n",
    "y_yes_train = yes_labels[:124]\n",
    "y_yes_valid = yes_labels[124:]\n",
    "\n",
    "y_no_train = no_labels[:78]\n",
    "y_no_valid = no_labels[78:]\n",
    "\n",
    "y_train = np.concatenate([y_yes_train, y_no_train])\n",
    "y_valid = np.concatenate([y_yes_valid, y_no_valid])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Create the CNN model\n",
    "\n",
    "Prepare the CNN Sequential model with 3 convolutional layers and 2 dense layers. Use the `relu` activation function for the convolutional layers and `sigmoid` activation function for the last dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=9,\n",
    "          padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.45))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=16, kernel_size=9, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=36, kernel_size=9, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.15))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.summary() # Uncomment to see the model summary. Model summary is already been shown the cell below in SVG representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Visualize the model summary\n",
    "Visualize the SVG representation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compile the model\n",
    "Use the `binary_crossentropy` loss function and `adam` optimizer to compile model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Train the model on the training data\n",
    "Train the model for 200 epochs with a batch size of 128. Use the `validation_data` parameter to pass the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=128,\n",
    "          epochs=200,\n",
    "          validation_data=(x_valid, y_valid),)\n",
    "\n",
    "# Saving the trained model\n",
    "model.save('brain_tumor_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Evaluate the model\n",
    "Evaluate the model on the validation data and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_valid, y_valid, verbose=\"0\")\n",
    "\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predicting the output\n",
    "  1. Predict the output for the validation data\n",
    "  2. Visualize the predicted output\n",
    "\n",
    "Plot a random sample of 15 test images, their predicted labels and ground truth with their accuracy scores\n",
    "Also, plot the original MRIs of those randomly selected images for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"No\", \"Yes\"]\n",
    "y_hat = model.predict(x_valid)\n",
    "\n",
    "no_of_prediction_samples = 15\n",
    "random_indices = np.random.choice(\n",
    "    x_valid.shape[0], size=no_of_prediction_samples, replace=False)\n",
    "# Plot a random sample of 15 test images, with their predicted labels and ground truth\n",
    "figure = plt.figure(figsize=(20, 13))\n",
    "sub_title = \"Random samples of 15 test images, with their predicted labels and ground truth\"\n",
    "figure.suptitle(sub_title, fontsize=20)\n",
    "for i in range(no_of_prediction_samples):\n",
    "    rand_index = random_indices[i]\n",
    "\n",
    "    # Display each image\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(x_valid[rand_index]))\n",
    "\n",
    "    # Set the title for each image\n",
    "    prediction_val = y_hat[rand_index][0]\n",
    "    predict_index = 0 if (prediction_val < 0.5) else 1\n",
    "    true_index = y_valid[rand_index]\n",
    "    prediction = labels[predict_index]\n",
    "    truth = labels[true_index]\n",
    "    title_color = \"blue\" if predict_index == true_index else \"red\"\n",
    "    ax_title = \"Prediction: {} ({:.2f})\\nGround Truth: {}\".format(\n",
    "        prediction, prediction_val, truth)\n",
    "    ax.set_title(ax_title, color=title_color)\n",
    "plt.show()\n",
    "\n",
    "# Show the Original MRIs of the randomly selected images\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "figure.suptitle(\"Original MRIs of those randomly selected images\", fontsize=20)\n",
    "for i in range(no_of_prediction_samples):\n",
    "    # get the original image\n",
    "    img = x_orig_valid[random_indices[i]]\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(img))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
